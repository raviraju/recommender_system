{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from fnmatch import fnmatch\n",
    "from pathlib import Path\n",
    "from os.path import basename\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "eval_results_datasets_dict = dict()\n",
    "for dataset in os.listdir('../models_datasets'):\n",
    "    eval_results_dataset_dict = dict()\n",
    "    for root_dir, sub_dir, file_names in os.walk('../models_datasets/' + dataset):\n",
    "        for file_name in file_names:\n",
    "            if fnmatch(file_name, 'evaluation_results.json'):\n",
    "                file_path = os.path.join(root_dir, file_name)\n",
    "                model_name = basename(Path(file_path).parent).split('hold_all_')[-1]\n",
    "                #print(model_name)\n",
    "\n",
    "                with open(file_path, 'r') as fp:\n",
    "                    eval_results = json.load(fp)\n",
    "                    #pprint(eval_results)                \n",
    "                    eval_results_dataset_dict[model_name] = eval_results\n",
    "    eval_results_datasets_dict[dataset] = eval_results_dataset_dict\n",
    "\n",
    "# eval_results_datasets_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "no_of_items_to_recommend = 10\n",
    "\n",
    "datasets_eval_results = []\n",
    "for dataset in eval_results_datasets_dict:\n",
    "    eval_results_dataset_dict = eval_results_datasets_dict[dataset]\n",
    "    dataset_eval = dict()\n",
    "    dataset_eval['dataset'] = dataset\n",
    "    for model in eval_results_dataset_dict:\n",
    "        #print(dataset)\n",
    "        eval_metrics = eval_results_dataset_dict[model]['no_of_items_to_recommend'][str(no_of_items_to_recommend)]\n",
    "        \n",
    "        for eval_metric, score in eval_metrics.items():\n",
    "            model_eval_metric = model + '_' + eval_metric\n",
    "            # print(model_eval_metric, score)\n",
    "            dataset_eval[model_eval_metric] = score\n",
    "    datasets_eval_results.append(dataset_eval)\n",
    "datasets_eval_results_df = pd.DataFrame(datasets_eval_results)\n",
    "# datasets_eval_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_eval_results_df.sort_values('dataset', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_quarters_dict = {\n",
    "    'dataset_1' : '2016Q2',\n",
    "    'dataset_2' : '2016Q3',\n",
    "    'dataset_3' : '2016Q4',\n",
    "    'dataset_4' : '2017Q1',\n",
    "}\n",
    "datasets_eval_results_df['quarter'] = datasets_eval_results_df['dataset'].map(dataset_quarters_dict)\n",
    "datasets_eval_results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     8
    ]
   },
   "outputs": [],
   "source": [
    "def get_eval_results(metric):\n",
    "    #metric_cols = ['dataset']\n",
    "    metric_cols = ['quarter']\n",
    "    for col in datasets_eval_results_df.columns:\n",
    "        if metric in col:\n",
    "            metric_cols.append(col)\n",
    "    return datasets_eval_results_df[metric_cols]\n",
    "\n",
    "def plot_eval_results(metric):\n",
    "    metric_df = get_eval_results(metric)\n",
    "    for col in metric_df.columns:\n",
    "        #if col == 'dataset':\n",
    "        #    continue\n",
    "        if col == 'quarter':\n",
    "            continue\n",
    "        model_name, metric = col.split('_avg_')\n",
    "        #plt.plot('dataset', col, data=metric_df, marker='o', label=model_name)\n",
    "        plt.plot('quarter', col, data=metric_df, marker='o', label=model_name)\n",
    "    plt.legend()  \n",
    "    #plt.xlabel('Datasets')\n",
    "    plt.xlabel('Quarter')\n",
    "    plt.ylabel(metric)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_eval_results('f1_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_eval_results('precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_eval_results('recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_eval_results('mcc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     8
    ]
   },
   "outputs": [],
   "source": [
    "def get_roc2_results():\n",
    "    #metric_cols = ['dataset']\n",
    "    metric_cols = ['quarter']\n",
    "    for col in datasets_eval_results_df.columns:\n",
    "        if 'tpr' in col or 'fpr' in col:\n",
    "            metric_cols.append(col)\n",
    "    return datasets_eval_results_df[metric_cols]\n",
    "\n",
    "def plot_roc2_results():\n",
    "    metric_df = get_roc2_results()\n",
    "    model_tpr_fpr = dict()\n",
    "    for col in metric_df.columns:     \n",
    "        if col == 'quarter':\n",
    "            continue\n",
    "        model_name, metric = col.split('_avg_')\n",
    "        if model_name not in model_tpr_fpr:\n",
    "            model_tpr_fpr[model_name] = dict()\n",
    "        temp = pd.DataFrame(metric_df[['quarter', col]]).set_index('quarter').to_dict()\n",
    "        model_tpr_fpr[model_name][metric] = temp[col]\n",
    "    #pprint(model_tpr_fpr)\n",
    "    for model in model_tpr_fpr:     \n",
    "        plt.plot(list(model_tpr_fpr[model]['fpr'].values()), list(model_tpr_fpr[model]['tpr'].values()), \n",
    "                 marker='o', label=model)    \n",
    "    plt.legend()  \n",
    "    plt.title('ROC2')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_roc2_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rec_sys_env",
   "language": "python",
   "name": "rec_sys_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
